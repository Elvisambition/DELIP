# DLIP
DLIP: Dense Contrastive Language-Image Pre-Training - Official Implementation

## News ðŸ“¢

- **[2023-12-23]** - ðŸš€ Launched the DLIP code repository! We are currently organizing the code and data for easy open-source access. Additionally, we are excited to announce that our paper detailing the DLIP project is in the final stages of preparation and will be published soon. Stay tuned! ðŸŒŸ



## Introduction

In this project, we have gathered a large number of high-quality `<image, dense caption>` pairs. By initializing with the CLIP model, we've conducted continue pre-training to enhance model's capabilities. A key improvement in our method is increasing the token limit from 77 to 2k. This expansion allows our model to handle more detailed and precise alignments between images and text.

## Citation

If you find DLIP useful in your research, please consider citing our work:

```bibtex
@misc{DLIP2023,
  title={DLIP: Dense Contrastive Language-Image Pre-Training},
  author={Zhiyuan Fan and Zhihong Chen and Benyou Wang},
  year={2023},
  publisher={GitHub},
  journal={GitHub repository},
  howpublished={\url{https://github.com/Elvisambition/DLIP}}
}

```
